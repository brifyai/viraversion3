import { NextRequest, NextResponse } from 'next/server'
import { getCurrentUser } from '@/lib/supabase-auth'
import { supabaseAdmin } from '@/lib/supabase-server'
import { getResourceOwnerId } from '@/lib/resource-owner'

// Constantes de ScrapingBee
const SCRAPINGBEE_API_KEY = process.env.SCRAPINGBEE_API_KEY
const SCRAPINGBEE_BASE_URL = 'https://app.scrapingbee.com/api/v1/'

interface NewsPreview {
    id: string
    titulo: string
    bajada: string
    url: string
    categoria: string
    fuente: string
    fuente_id: string
    imagen_url?: string
    fecha_publicacion?: string  // ‚úÖ Fecha extra√≠da de la URL
}

// Funci√≥n para categorizar noticias autom√°ticamente
function categorizarNoticia(titulo: string, bajada: string = ''): string {
    const texto = `${titulo} ${bajada}`.toLowerCase()

    // Orden de prioridad: espec√≠fico a gen√©rico
    const categorias: { [key: string]: string[] } = {
        // Regionales primero (m√°s espec√≠fico) - incluye regiones y ciudades de Chile
        'Regionales': [
            'regi√≥n', 'regional', 'provincial', 'comunal', 'municipio', 'alcalde', 'gobernador', 'local',
            '√±uble', 'chill√°n', 'concepci√≥n', 'biob√≠o', 'bio-bio', 'talca', 'maule', 'valpara√≠so',
            'vi√±a del mar', 'antofagasta', 'temuco', 'araucan√≠a', 'puerto montt', 'los lagos',
            'coquimbo', 'la serena', 'rancagua', 'o\'higgins', 'arica', 'iquique', 'punta arenas',
            'magallanes', 'ays√©n', 'los r√≠os', 'valdivia', 'osorno', 'atacama', 'copiap√≥'
        ],
        'Deportes': [
            'f√∫tbol', 'futbol', 'gol', 'partido', 'estadio', 'selecci√≥n', 'colo colo', 'colo-colo',
            'universidad de chile', 'universidad cat√≥lica', 'liga', 'copa', 'deportes', 'jugador',
            't√©cnico', 'entrenador', 'campeonato', 'atleta', 'ol√≠mpico', 'mundial', 'champions',
            'libertadores', 'sudamericana', 'tenis', 'nadal', 'arturo vidal', 'alexis s√°nchez'
        ],
        'Pol√≠tica': [
            'gobierno', 'presidente', 'ministro', 'congreso', 'senado', 'diputado', 'elecciones',
            'votaci√≥n', 'pol√≠tico', 'ley', 'decreto', 'boric', 'pi√±era', 'bachelet', 'carabineros',
            'pdi', 'fiscal√≠a', 'fiscal', 'tribunal', 'suprema', 'constitucional', 'parlamentario',
            'izquierda', 'derecha', 'oposici√≥n', 'oficialismo', 'reforma', 'proyecto de ley'
        ],
        'Econom√≠a': [
            'd√≥lar', 'econom√≠a', 'banco', 'imacec', 'inflaci√≥n', 'mercado', 'bolsa', 'inversi√≥n',
            'finanzas', 'empresas', 'comercio', 'pib', 'cobre', 'miner√≠a', 'exportaciones',
            'importaciones', 'afp', 'pensiones', 'sueldo', 'empleo', 'desempleo', 'precio',
            'uf', 'ipsa', 'sii', 'impuestos', 'bce', 'banco central', 'recesi√≥n', 'crecimiento'
        ],
        'Mundo': [
            'internacional', 'eeuu', 'estados unidos', 'china', 'rusia', 'ucrania', 'europa',
            'brasil', 'argentina', 'per√∫', 'bolivia', 'extranjero', 'trump', 'biden', 'putin',
            'onu', 'otan', 'uni√≥n europea', 'medio oriente', 'israel', 'palestina', 'gaza',
            'venezuela', 'maduro', 'guerra', 'conflicto internacional'
        ],
        'Tecnolog√≠a': [
            'tecnolog√≠a', 'apple', 'google', 'microsoft', 'inteligencia artificial', 'ia', 'openai',
            'chatgpt', 'smartphone', 'app', 'digital', 'internet', 'ciberseguridad', 'bitcoin',
            'criptomonedas', 'elon musk', 'tesla', 'meta', 'facebook', 'starlink', 'innovaci√≥n',
            'startup', 'software', 'datos', 'privacidad digital'
        ],
        'Tendencias': [
            'viral', 'redes sociales', 'instagram', 'tiktok', 'twitter', 'x.com', 'far√°ndula',
            'espect√°culo', 'celebridad', 'influencer', 'youtuber', 'streaming', 'netflix',
            'trending', 'meme', 'cultura pop'
        ],
        // Nacionales al final (m√°s gen√©rico, act√∫a como fallback)
        'Nacionales': [
            'chile', 'chileno', 'chilena', 'santiago', 'nacional', 'pa√≠s', 'la moneda',
            'metro de santiago', 'transantiago', 'red metropolitana'
        ]
    }

    for (const [categoria, keywords] of Object.entries(categorias)) {
        if (keywords.some(keyword => texto.includes(keyword))) {
            return categoria
        }
    }

    return 'Nacionales' // Default
}

// Scrapea la p√°gina principal de una fuente con ScrapingBee
// ‚úÖ OPTIMIZADO: Verifica cache antes de scrapear para ahorrar cr√©ditos
async function scanSourceHomepage(source: { id: string, url: string, nombre_fuente: string }): Promise<{ noticias: NewsPreview[], fromCache: boolean }> {
    if (!SCRAPINGBEE_API_KEY) {
        console.error('‚ùå SCRAPINGBEE_API_KEY no configurada')
        return { noticias: [], fromCache: false }
    }

    try {
        // ‚úÖ PASO 1: Verificar si hay cache v√°lido
        const { data: cached } = await supabaseAdmin
            .from('scraping_cache')
            .select('*')
            .eq('fuente_url', source.url)
            .gt('expires_at', new Date().toISOString())
            .single()

        if (cached && cached.noticias && Array.isArray(cached.noticias)) {
            console.log(`üì¶ Usando CACHE para ${source.nombre_fuente} (${cached.noticias.length} noticias, expira: ${new Date(cached.expires_at).toLocaleTimeString()})`)

            // Convertir noticias del cache al formato esperado
            const noticiasFromCache: NewsPreview[] = cached.noticias.map((n: any) => ({
                id: n.id || `cache-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
                titulo: n.titulo,
                bajada: n.bajada || '',
                url: n.url,
                categoria: n.categoria,
                fuente: source.nombre_fuente,
                fuente_id: source.id,
                imagen_url: n.imagen_url,
                fecha_publicacion: n.fecha_publicacion
            }))

            return { noticias: noticiasFromCache, fromCache: true }
        }

        // ‚úÖ PASO 2: No hay cache, scrapear normalmente
        console.log(`üîç Escaneando: ${source.nombre_fuente} - ${source.url} (sin cache)`)

        const params = new URLSearchParams({
            api_key: SCRAPINGBEE_API_KEY,
            url: source.url,
            render_js: 'true'
        })

        const response = await fetch(`${SCRAPINGBEE_BASE_URL}?${params.toString()}`)

        if (!response.ok) {
            console.error(`‚ùå Error ScrapingBee: ${response.status}`)
            return { noticias: [], fromCache: false }
        }

        const html = await response.text()

        // Parsear HTML para extraer noticias
        const noticias = parseNewsFromHTML(html, source)
        console.log(`‚úÖ Encontradas ${noticias.length} noticias en ${source.nombre_fuente}`)

        // ‚úÖ PASO 3: Guardar en cache para futuros requests
        if (noticias.length > 0) {
            const categoriasConteo: { [key: string]: number } = {}
            noticias.forEach(n => {
                categoriasConteo[n.categoria] = (categoriasConteo[n.categoria] || 0) + 1
            })

            const { error: cacheError } = await supabaseAdmin
                .from('scraping_cache')
                .upsert({
                    fuente_id: source.id,
                    fuente_url: source.url,
                    noticias: noticias.map(n => ({
                        id: n.id,
                        titulo: n.titulo,
                        bajada: n.bajada,
                        url: n.url,
                        categoria: n.categoria,
                        imagen_url: n.imagen_url,
                        fecha_publicacion: n.fecha_publicacion
                    })),
                    categorias_conteo: categoriasConteo,
                    total_noticias: noticias.length,
                    created_at: new Date().toISOString(),
                    expires_at: new Date(Date.now() + 24 * 60 * 60 * 1000).toISOString() // 24 horas
                }, {
                    onConflict: 'fuente_url'
                })

            if (cacheError) {
                console.warn(`‚ö†Ô∏è Error guardando cache: ${cacheError.message}`)
            } else {
                console.log(`üíæ Cache guardado para ${source.nombre_fuente} (expira en 24h)`)
            }
        }

        return { noticias, fromCache: false }

    } catch (error) {
        console.error(`‚ùå Error escaneando ${source.nombre_fuente}:`, error)
        return { noticias: [], fromCache: false }
    }
}

// ‚úÖ Funci√≥n para extraer fecha de la URL (patr√≥n com√∫n en sitios de noticias)
function extractDateFromUrl(url: string): Date | null {
    // Patrones comunes de fechas en URLs
    const patterns = [
        // /2024/12/12/ o /2024-12-12/
        /\/(\d{4})[-\/](\d{1,2})[-\/](\d{1,2})\//,
        // /12-12-2024/ o /12/12/2024/
        /\/(\d{1,2})[-\/](\d{1,2})[-\/](\d{4})\//,
        // ?date=2024-12-12
        /date=(\d{4})-(\d{1,2})-(\d{1,2})/,
        // /noticias/2024/diciembre/12/
        /\/(\d{4})\/(enero|febrero|marzo|abril|mayo|junio|julio|agosto|septiembre|octubre|noviembre|diciembre)\/(\d{1,2})\//i,
    ]

    for (const pattern of patterns) {
        const match = url.match(pattern)
        if (match) {
            let year, month, day

            if (match[2] && isNaN(parseInt(match[2]))) {
                // Es un nombre de mes
                const monthNames: { [key: string]: number } = {
                    'enero': 0, 'febrero': 1, 'marzo': 2, 'abril': 3,
                    'mayo': 4, 'junio': 5, 'julio': 6, 'agosto': 7,
                    'septiembre': 8, 'octubre': 9, 'noviembre': 10, 'diciembre': 11
                }
                year = parseInt(match[1])
                month = monthNames[match[2].toLowerCase()]
                day = parseInt(match[3])
            } else if (parseInt(match[1]) > 31) {
                // Formato YYYY/MM/DD
                year = parseInt(match[1])
                month = parseInt(match[2]) - 1
                day = parseInt(match[3])
            } else {
                // Formato DD/MM/YYYY
                day = parseInt(match[1])
                month = parseInt(match[2]) - 1
                year = parseInt(match[3])
            }

            if (year && month !== undefined && day) {
                return new Date(year, month, day)
            }
        }
    }

    return null
}

// ‚úÖ Funci√≥n para verificar si una fecha es reciente (m√°ximo N d√≠as de antig√ºedad)
function isNewsRecent(dateFromUrl: Date | null, maxDaysOld: number = 2): boolean {
    if (!dateFromUrl) return true // Si no hay fecha, asumimos que es reciente

    const now = new Date()
    const diffTime = now.getTime() - dateFromUrl.getTime()
    const diffDays = diffTime / (1000 * 60 * 60 * 24)

    return diffDays <= maxDaysOld
}

// Parser gen√©rico de noticias desde HTML
function parseNewsFromHTML(html: string, source: { id: string, url: string, nombre_fuente: string }): NewsPreview[] {
    const noticias: NewsPreview[] = []

    // Usar regex para extraer noticias (compatible con servidor sin DOM)
    // Patr√≥n para encontrar art√≠culos/links de noticias

    // Buscar patrones comunes de titulares
    const patterns = [
        // Patr√≥n 1: <h2><a href="URL">TITULO</a></h2>
        /<h[1-3][^>]*>\s*<a[^>]*href=["']([^"']+)["'][^>]*>([^<]+)<\/a>/gi,
        // Patr√≥n 2: <a href="URL" class="...title...">TITULO</a>
        /<a[^>]*href=["']([^"']+)["'][^>]*class=["'][^"']*(?:title|headline|titular)[^"']*["'][^>]*>([^<]+)<\/a>/gi,
        // Patr√≥n 3: <article><a href="URL">...<h2>TITULO</h2></a></article>
        /<article[^>]*>[\s\S]*?<a[^>]*href=["']([^"']+)["'][^>]*>[\s\S]*?<h[1-3][^>]*>([^<]+)<\/h[1-3]>/gi,
    ]

    const seenUrls = new Set<string>()
    const baseUrl = new URL(source.url).origin
    const MAX_DAYS_OLD = 2 // Solo noticias de los √∫ltimos 2 d√≠as

    for (const pattern of patterns) {
        let match
        while ((match = pattern.exec(html)) !== null) {
            let [, url, titulo] = match

            // Normalizar URL
            if (url.startsWith('/')) {
                url = baseUrl + url
            }

            // Filtrar URLs duplicadas y no-noticias
            if (seenUrls.has(url)) continue
            if (!url.includes('http')) continue
            if (url.includes('#') || url.includes('javascript:')) continue
            if (titulo.length < 20 || titulo.length > 300) continue

            // ‚úÖ NUEVO: Extraer fecha de la URL y filtrar viejas
            const dateFromUrl = extractDateFromUrl(url)
            if (!isNewsRecent(dateFromUrl, MAX_DAYS_OLD)) {
                console.log(`‚è∞ Noticia vieja filtrada (${dateFromUrl?.toLocaleDateString()}): ${titulo.substring(0, 50)}...`)
                continue
            }

            // Limpiar t√≠tulo
            titulo = titulo.trim().replace(/\s+/g, ' ')

            seenUrls.add(url)

            const categoria = categorizarNoticia(titulo)

            noticias.push({
                id: `preview-${Date.now()}-${noticias.length}`,
                titulo,
                bajada: '', // Se obtiene en scraping profundo
                url,
                categoria,
                fuente: source.nombre_fuente,
                fuente_id: source.id,
                // ‚úÖ NUEVO: Agregar fecha extra√≠da de URL (si existe)
                fecha_publicacion: dateFromUrl?.toISOString()
            })

            if (noticias.length >= 50) break // L√≠mite por fuente
        }
        if (noticias.length >= 50) break
    }

    return noticias
}

// Helper para obtener fuentes del usuario (con filtro opcional por regi√≥n)
async function getUserSources(
    resourceOwnerId: string,
    filterRegion?: string
): Promise<{ id: string, url: string, nombre_fuente: string, region: string }[]> {
    try {
        const { data: suscripciones, error: subError } = await supabaseAdmin
            .from('user_fuentes_suscripciones')
            .select(`
                id,
                categoria,
                fuente:fuentes_final (
                    id,
                    url,
                    nombre_fuente,
                    region,
                    esta_activo
                )
            `)
            .eq('user_id', resourceOwnerId)
            .eq('esta_activo', true)

        if (!subError && suscripciones && suscripciones.length > 0) {
            let fuentes = suscripciones
                .filter((s: any) => s.fuente?.esta_activo)
                .map((s: any) => ({
                    id: s.fuente.id,
                    url: s.fuente.url,
                    nombre_fuente: s.fuente.nombre_fuente,
                    region: s.fuente.region
                }))

            // ‚úÖ Filtrar por regi√≥n si se especifica
            // Incluir fuentes de la regi√≥n espec√≠fica + fuentes "Nacional" (siempre √∫tiles)
            if (filterRegion && filterRegion !== 'Nacional') {
                const regionLower = filterRegion.toLowerCase()
                fuentes = fuentes.filter(f =>
                    f.region.toLowerCase() === regionLower ||
                    f.region.toLowerCase() === 'nacional'
                )
                console.log(`üåç Filtradas ${fuentes.length} fuentes para regi√≥n: ${filterRegion}`)
            }

            return fuentes
        }

        // Sin suscripciones = sin fuentes (el usuario debe agregar desde /activos)
        console.log(`‚ö†Ô∏è Usuario ${resourceOwnerId} no tiene fuentes suscritas`)
        return []

    } catch (error) {
        console.error('Error obteniendo fuentes del usuario:', error)
        return []
    }
}

export async function GET(request: NextRequest) {
    try {
        const user = await getCurrentUser()
        if (!user) {
            return NextResponse.json({ error: 'No autorizado' }, { status: 401 })
        }

        const resourceOwnerId = getResourceOwnerId(user)
        const fuentes = await getUserSources(resourceOwnerId)

        return NextResponse.json({
            success: true,
            fuentes
        })
    } catch (error) {
        console.error('Error en GET sources:', error)
        return NextResponse.json({ error: 'Error obteniendo fuentes' }, { status: 500 })
    }
}

export async function POST(request: NextRequest) {
    try {
        // Autenticar usuario usando getCurrentUser (funciona con cookies)
        const user = await getCurrentUser()
        if (!user) {
            return NextResponse.json({ error: 'No autorizado' }, { status: 401 })
        }

        const resourceOwnerId = getResourceOwnerId(user)

        // Obtener body para ver si hay filtro de fuentes y regi√≥n
        let sourceIds: string[] | undefined
        let filterRegion: string | undefined
        try {
            const body = await request.json()
            if (body.sourceIds && Array.isArray(body.sourceIds)) {
                sourceIds = body.sourceIds
            }
            // ‚úÖ NUEVO: Aceptar regi√≥n para filtrar fuentes
            if (body.region && typeof body.region === 'string') {
                filterRegion = body.region
                console.log(`üåç Regi√≥n para filtrar: ${filterRegion}`)
            }
        } catch (e) {
            // Body vac√≠o es v√°lido (escanear todas)
        }

        // Obtener fuentes disponibles (filtradas por regi√≥n si se especifica)
        let fuentes = await getUserSources(resourceOwnerId, filterRegion)

        if (fuentes.length === 0) {
            return NextResponse.json({
                success: false,
                error: 'No hay fuentes de noticias configuradas en el sistema.',
                noticias: [],
                por_categoria: {}
            })
        }

        // Filtrar si se especificaron IDs
        if (sourceIds && sourceIds.length > 0) {
            console.log(`üéØ Filtrando por ${sourceIds.length} fuentes seleccionadas`)
            fuentes = fuentes.filter(f => sourceIds!.includes(f.id))
        }

        if (fuentes.length === 0) {
            return NextResponse.json({
                success: false,
                error: 'No hay fuentes seleccionadas para escanear',
                noticias: [],
                por_categoria: {}
            })
        }

        console.log(`üì° Escaneando ${fuentes.length} fuentes para usuario ${user.email}`)

        // Escanear cada fuente en paralelo (m√°x 3 a la vez)
        const allNoticias: NewsPreview[] = []
        const batchSize = 3
        let cachedSources = 0
        let freshSources = 0

        for (let i = 0; i < fuentes.length; i += batchSize) {
            const batch = fuentes.slice(i, i + batchSize)
            const results = await Promise.all(batch.map(scanSourceHomepage))
            results.forEach(result => {
                allNoticias.push(...result.noticias)
                if (result.fromCache) {
                    cachedSources++
                } else {
                    freshSources++
                }
            })
        }

        // Contar por categor√≠a
        const porCategoria: { [key: string]: number } = {}
        allNoticias.forEach(n => {
            porCategoria[n.categoria] = (porCategoria[n.categoria] || 0) + 1
        })

        console.log(`‚úÖ Total: ${allNoticias.length} noticias encontradas`)
        console.log(`üìä Por categor√≠a:`, porCategoria)
        console.log(`üíæ Fuentes de cache: ${cachedSources} | Scrapeadas: ${freshSources}`)

        return NextResponse.json({
            success: true,
            noticias: allNoticias,
            por_categoria: porCategoria,
            fuentes_escaneadas: fuentes.length,
            total_noticias: allNoticias.length,
            cached_sources: cachedSources,
            fresh_sources: freshSources
        })

    } catch (error: any) {
        console.error('Error en preview scraping:', error)
        return NextResponse.json(
            { error: error.message || 'Error interno del servidor' },
            { status: 500 }
        )
    }
}
